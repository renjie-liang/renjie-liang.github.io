---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}


# About Me
<span class='anchor' id='about-me'></span>
I am Renjie Liang, an incoming PhD student at the University of Florida. I served as a research associate at **Nanyang Technological University**, collaborating with Prof. [Aixin Sun](https://scholar.google.com/citations?user=wyKGVKUAAAAJ&hl=zh-CN). Prior to that, I worked as a research intern under the guidance of Prof. [Tat-Seng Chua](https://scholar.google.com/citations?user=Z9DWCBEAAAAJ&hl=zh-CN&oi=ao) and Post PhD. [Wei Ji](https://jiwei0523.github.io/).
I completed my Master's degree at **Sun Yat-sen University**, China in 2020, and my Bachelor's degree at **Central South University**, China in 2017. My research interests are centered around information retrieval, multimodal understanding, and AI for healthcare.


# Work Experiences
<span class='anchor' id='work-experiences'></span>

<div style="display: flex; justify-content: space-between; align-items: center; width: 100%;">
    <span style="margin: 0; font-weight: bold; flex-grow: 1;">2023. April - Present</span>
    <span style="margin: 0; font-weight: bold; flex-grow: 1; text-align: right;">Research Associate, S-Lab, NTU</span>
</div>

- Supervised by Prof. [Aixin Sun](https://scholar.google.com/citations?user=wyKGVKUAAAAJ&hl=zh-CN)
- Proposed a novel task, Ranked Video Moment Retrieval, and curated the TVR-Ranking dataset.
      

<div style="display: flex; justify-content: space-between; align-items: center;">
    <span style="margin: 0; font-weight: bold;">2022. March - 2023. April</span>
    <span style="text-align: right; font-weight: bold;">Research Intern, NExT++, NUS</span>
</div>

- Under the guidance of Prof. [Tat-Seng Chua](https://scholar.google.com/citations?user=Z9DWCBEAAAAJ&hl=zh-CN&oi=ao) and Postdoctoral Fellow [Wei Ji](https://jiwei0523.github.io/).
- Focused on information retrieval for multimodal data.

         


# Publications
<span class='anchor' id='publications'></span>
please refer to [google scholar](https://scholar.google.com/citations?user=r4kIL4cAAAAJ&hl=zh-CN) page to check all my publications. (* Equal Contribution)

<table style="MARGIN-BOTTOM: 10px; FONT-SIZE: 13px; BORDER-COLLAPSE: collapse; TEXT-ALIGN: left; WIDTH: 98%; BACKGROUND-COLOR: #f6fbfe">
  <tbody>
  <tr>
    <td class="left" style="FONT-SIZE: 10px; TEXT-ALIGN: center; WIDTH: 60px; BACKGROUND-COLOR: #e2eff9"><a href="https://arxiv.org/pdf/2406.01601" target="_blank"><img src="./images/pdf.png" width="100" height="100"></a></td>
    <td><span class="title" style="FONT-WEIGHT: bold">Backpropogation-Free Multi-modal On-Device Model Adaptation via Cloud-Device Collaboration</span> 
      <br>Wei Ji*, <b>Li Li</b>*, Zheqi Lv*, Wenqiao Zhang, Mengze Li, Zhen Wan, Wenqiang Lei, Roger Zimmermann
    <br>Preprint&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table style="MARGIN-BOTTOM: 10px; FONT-SIZE: 13px; BORDER-COLLAPSE: collapse; TEXT-ALIGN: left; WIDTH: 98%; BACKGROUND-COLOR: #f6fbfe">
  <tbody>
  <tr>
    <td class="left" style="FONT-SIZE: 10px; TEXT-ALIGN: center; WIDTH: 60px; BACKGROUND-COLOR: #e2eff9"><a href="https://arxiv.org/pdf/2309.17205.pdf" target="_blank"><img src="./images/pdf.png" width="100" height="100"></a></td>
    <td><span class="title" style="FONT-WEIGHT: bold">Towards Complex-query Referring Image Segmentation: A Novel Benchmark</span> 
      <br>Wei Ji, <b>Li Li</b>, Hao Fei, Xiangyan Liu, Xun Yang, Juncheng Li, Roger Zimmermann
    <br>Preprint&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table style="MARGIN-BOTTOM: 10px; FONT-SIZE: 13px; BORDER-COLLAPSE: collapse; TEXT-ALIGN: left; WIDTH: 98%; BACKGROUND-COLOR: #f6fbfe">
  <tbody>
  <tr>
    <td class="left" style="FONT-SIZE: 10px; TEXT-ALIGN: center; WIDTH: 60px; BACKGROUND-COLOR: #e2eff9"><a href="https://arxiv.org/pdf/2308.09412.pdf" target="_blank"><img src="./images/pdf.png" width="100" height="100"></a></td>
    <td><span class="title" style="FONT-WEIGHT: bold">Causal SAR ATR with Limited Data via Dual Invariance</span> 
      <br>Chenwei Wang, You Qin, <b>Li Li</b>, Siyi Luo, Yulin Huang, Jifang Pei, Yin Zhang, Jianyu Yang
    <br>Preprint&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table style="MARGIN-BOTTOM: 10px; FONT-SIZE: 13px; BORDER-COLLAPSE: collapse; TEXT-ALIGN: left; WIDTH: 98%; BACKGROUND-COLOR: #f6fbfe">
  <tbody>
  <tr>
    <td class="left" style="FONT-SIZE: 10px; TEXT-ALIGN: center; WIDTH: 60px; BACKGROUND-COLOR: #e2eff9"><a href="https://arxiv.org/pdf/2308.03725.pdf" target="_blank"><img src="./images/pdf.png" width="100" height="100"></a></td>
    <td><span class="title" style="FONT-WEIGHT: bold">Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation</span> 
      <br>Renjie Liang, Yiming Yang, Hui Lu, <b>Li Li</b>
    <br>Preprint&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table style="MARGIN-BOTTOM: 10px; FONT-SIZE: 13px; BORDER-COLLAPSE: collapse; TEXT-ALIGN: left; WIDTH: 98%; BACKGROUND-COLOR: #f6fbfe">
  <tbody>
  <tr>
    <td class="left" style="FONT-SIZE: 10px; TEXT-ALIGN: center; WIDTH: 60px; BACKGROUND-COLOR: #e2eff9"><a href="https://ojs.aaai.org/index.php/AAAI/article/download/28098/28201" target="_blank"><img src="./images/pdf.png" width="100" height="100"></a></td>
    <td><span class="title" style="FONT-WEIGHT: bold">Panoptic Scene Graph Generation with Semantics-prototype Learning</span> 
      <br><b>Li Li</b>, Wei Ji, Yiming Wu, Mengze Li, You Qin, Lina Wei, Roger Zimmermann
    <br>AAAI 2024&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table style="MARGIN-BOTTOM: 10px; FONT-SIZE: 13px; BORDER-COLLAPSE: collapse; TEXT-ALIGN: left; WIDTH: 98%; BACKGROUND-COLOR: #f6fbfe">
  <tbody>
  <tr>
    <td class="left" style="FONT-SIZE: 10px; TEXT-ALIGN: center; WIDTH: 60px; BACKGROUND-COLOR: #e2eff9"><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10447193" target="_blank"><img src="./images/pdf.png" width="100" height="100"></a></td>
    <td><span class="title" style="FONT-WEIGHT: bold">Domain-wise Invariant Learning for Panoptic Scene Graph Generation</span> 
      <br><b>Li Li</b>, You Qin, Wei Ji, Yuxiao Zhou, Roger Zimmermann
    <br>ICASSP 2024&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table style="MARGIN-BOTTOM: 10px; FONT-SIZE: 13px; BORDER-COLLAPSE: collapse; TEXT-ALIGN: left; WIDTH: 98%; BACKGROUND-COLOR: #f6fbfe">
  <tbody>
  <tr>
    <td class="left" style="FONT-SIZE: 10px; TEXT-ALIGN: center; WIDTH: 60px; BACKGROUND-COLOR: #e2eff9"><a href="https://dl.acm.org/doi/pdf/10.1145/3581783.3611847" target="_blank"><img src="./images/pdf.png" width="100" height="100"></a></td>
    <td><span class="title" style="FONT-WEIGHT: bold">Biased-Predicate Annotation Identification via Unbiased Visual Predicate Representation</span> 
      <br><b>Li Li</b>*, Chenwei Wang*, You Qin, Wei Ji, Renjie Liang
    <br>ACM MM 2023&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table style="MARGIN-BOTTOM: 10px; FONT-SIZE: 13px; BORDER-COLLAPSE: collapse; TEXT-ALIGN: left; WIDTH: 98%; BACKGROUND-COLOR: #f6fbfe">
  <tbody>
  <tr>
    <td class="left" style="FONT-SIZE: 10px; TEXT-ALIGN: center; WIDTH: 60px; BACKGROUND-COLOR: #e2eff9"><a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/407106f4b56040b2e8dcad75a6e461e5-Paper-Conference.pdf" target="_blank"><img src="./images/pdf.png" width="100" height="100"></a></td>
    <td><span class="title" style="FONT-WEIGHT: bold">VPGTrans: Transfer Visual Prompt Generator across LLMs</span> 
      <br>Ao Zhang, Hao Fei, Yuan Yao, Wei Ji, <b>Li Li</b>, Zhiyuan Liu, Tat-Seng Chua
    <br>NeurIPS 2023&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table style="MARGIN-BOTTOM: 10px; FONT-SIZE: 13px; BORDER-COLLAPSE: collapse; TEXT-ALIGN: left; WIDTH: 98%; BACKGROUND-COLOR: #f6fbfe">
  <tbody>
  <tr>
    <td class="left" style="FONT-SIZE: 10px; TEXT-ALIGN: center; WIDTH: 60px; BACKGROUND-COLOR: #e2eff9"><a href="https://arxiv.org/pdf/2202.06649.pdf" target="_blank"><img src="./images/pdf.png" width="100" height="100"></a></td>
    <td><span class="title" style="FONT-WEIGHT: bold">On the Importance of Building High-quality Training Datasets for Neural Code Search</span> 
      <br>Zhensu Sun, <b>Li Li</b>, Yan Liu, Xiaoning Du, Li Li
    <br>ICSE 2022, <span style="color: red">Nominated for distinguished paper award</span>&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>


<!-- <table style="MARGIN-BOTTOM: 10px; FONT-SIZE: 13px; BORDER-COLLAPSE: collapse; TEXT-ALIGN: left; WIDTH: 98%; BACKGROUND-COLOR: #f6fbfe">
  <tbody>
  <tr>
    <td class="left" style="FONT-SIZE: 10px; TEXT-ALIGN: center; WIDTH: 60px; BACKGROUND-COLOR: #e2eff9"><a href="https://lili0415.github.io" target="_blank"><img src="./images/pdf.png" width="100" height="100"></a></td>
    <td><span class="title" style="FONT-WEIGHT: bold">Generalized Video Moment Retrieval</span> 
      <br>Wei Ji, You Qin, Qilong Wu, <b>Li Li</b>, Pengcheng Cai, Lina Wei, Roger Zimmermann
    <br>Preprint&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table style="MARGIN-BOTTOM: 10px; FONT-SIZE: 13px; BORDER-COLLAPSE: collapse; TEXT-ALIGN: left; WIDTH: 98%; BACKGROUND-COLOR: #f6fbfe">
  <tbody>
  <tr>
    <td class="left" style="FONT-SIZE: 10px; TEXT-ALIGN: center; WIDTH: 60px; BACKGROUND-COLOR: #e2eff9"><a href="https://lili0415.github.io" target="_blank"><img src="./images/pdf.png" width="100" height="100"></a></td>
    <td><span class="title" style="FONT-WEIGHT: bold">StableSynthNet: Revolutionizing HyperNetworks for Enhanced Multi-modal Model Generalization</span> 
      <br>Wei Ji*, <b>Li Li</b>*, Zheqi Lv*, Wenqiao Zhang, Yifang Yin, Fei Wu, Roger Zimmermann
    <br>Preprint&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table style="MARGIN-BOTTOM: 10px; FONT-SIZE: 13px; BORDER-COLLAPSE: collapse; TEXT-ALIGN: left; WIDTH: 98%; BACKGROUND-COLOR: #f6fbfe">
  <tbody>
  <tr>
    <td class="left" style="FONT-SIZE: 10px; TEXT-ALIGN: center; WIDTH: 60px; BACKGROUND-COLOR: #e2eff9"><a href="https://lili0415.github.io" target="_blank"><img src="./images/pdf.png" width="100" height="100"></a></td>
    <td><span class="title" style="FONT-WEIGHT: bold">Backpropogation-Free Multi-modal On-Device Model Adaptation via Cloud-Device Collaboration</span> 
      <br>Wei Ji*, <b>Li Li</b>*, Zheqi Lv*, Wenqiao Zhang, Mengze Li, Zhen Wan, Wenqiang Lei, Roger Zimmermann
    <br>Preprint&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table> -->




<!-- - **On the Importance of Building High-quality Training Datasets for Neural Code Search**

  Zhensu Sun, <span style="border-bottom:2px solid black;">**Li Li**</span>, Yan Liu, Xiaoning Du, Li Li

  **ICSE 2022, Nominated for distinguished paper award**

- **Biased-Predicate Annotation Identification via Unbiased Visual Predicate Representation**

  <span style="border-bottom:2px solid black;">**Li Li**</span>\*, Chenwei Wang*, You Qin, Wei Ji, Renjie Liang

  **ACM MM 2023, Accepted with full marks**

- **Transfer Visual Prompt Generator across LLMs**

  Ao Zhang, Hao Fei, Yuan Yao, Wei Ji, <span style="border-bottom:2px solid black;">**Li Li**</span>, Zhiyuan Liu, Tat-Seng Chua

  **NeurIPS 2023**

- **Panoptic Scene Graph Generation with Semantics-prototype Learning**

  <span style="border-bottom:2px solid black;">**Li Li**</span>, Wei Ji, Yiming Wu, Mengze Li, You Qin, Lina Wei, Roger Zimmermann

  **AAAI 2024**

- **Domain-wise Invariant Learning for Panoptic Scene Graph Generation**

  <span style="border-bottom:2px solid black;">**Li Li**</span>, You Qin, Wei Ji, Yuxiao Zhou, Roger Zimmermann
  
  **ICASSP 2024** -->

<!-- - **StableSynthNet: Revolutionizing HyperNetworks for Enhanced Multi-modal Model Generalization**

  Wei Ji\*, <span style="border-bottom:2px solid black;">**Li Li**</span>\*, Zheqi Lv, Wenqiao Zhang, Yifang Yin, Fei Wu, Roger Zimmermann

  **Submitted to CVPR 2024** -->

<!-- 
- **Towards Complex-query Referring Image Segmentation: A Novel Benchmark**

  Wei Ji, <span style="border-bottom:2px solid black;">**Li Li**</span>, Hao Fei, Xiangyan Liu, Xun Yang, Juncheng Li, Roger Zimmermann

  **Submitted to IEEE T-MM** -->

<!-- - **Backpropogation-Free On-Device Multi-Modal Model Adaptation via Cloud-Device Collaboration**

  Wei Ji*, **Li Li\***, Zheqi Lv, Wenqiao Zhang, Mengze Li, Zhen Wan, Wenqiang Lei, Roger Zimmermann

  **Submitted to ACM Web Conference 2024** -->

# Honors and Awards
<span class='anchor' id='honors-and-awards'></span>
ICSE 2022 Distinguished Paper Award Nomination.

AAAI 2024 Student Scholarship.

# Services
<span class='anchor' id='services'></span>
Program Committee Member of ACM MM (2024) .

Program Committee Member of ECCV (2024).

Program Committee Member of ACL ARR (2024).

Program Committee Member of ICLR AGI Workshop (2024).

Program Committee Member of ACM MM MMGR Workshop (2023, 2024).

Student Volunteer at ACM Web Conference (2024).

# Educations
<span class='anchor' id='educations'></span>
  Incoming

  Doctor of Philosophy in Computer Science

  University of Southern California, USA


  ---


  2022.08 - 2024.02

  Master of Science in Industry 4.0

  National University of Singapore, Singapore


  ---
  
  2018.08 - 2022.06

  Bachelor of Engineering in Software Engineering

  TongJi University, Shanghai

<!-- <script type="text/javascript" id="mapmyvisitors" src="//mapmyvisitors.com/map.js?d=MtgOD5bYVhrJl1tzX74CbRhUUslEFdbq-StiPxMz5Ts&cl=ffffff&w=a"></script> -->




<!-- <script>
  var _paq = window._paq = window._paq || [];
  /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
  _paq.push(['trackPageView']);
  _paq.push(['enableLinkTracking']);
  (function() {
    var u="https://githublili.matomo.cloud/";
    _paq.push(['setTrackerUrl', u+'matomo.php']);
    _paq.push(['setSiteId', '1']);
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
    g.async=true; g.src='https://cdn.matomo.cloud/githublili.matomo.cloud/matomo.js'; s.parentNode.insertBefore(g,s);
  })();
</script> -->